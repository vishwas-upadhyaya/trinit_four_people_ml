{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aab005cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import ast\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from time import time\n",
    "\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\n",
    "# from tensorflow.keras.layers.wrappers import Bidirectional\n",
    "# from tensorflow.keras.layers.merge import add\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2481be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(datas):\n",
    "    return_data = []\n",
    "    for data in datas:\n",
    "        data=data.lower()\n",
    "        data=data.replace('\\\\r','').replace('\\\\n','').replace('\\\\',' ').replace(\"n\\'t\",' not').replace(',',' ')\\\n",
    "        .replace('.',' ').replace('%',' ').replace(\"'s\",' is').replace('-',' ').replace('\"','').replace('_','').replace(':','')\\\n",
    "        .replace(';','').replace('!','').replace('!!','').replace(';','').replace('/',' ').replace('?',' ')\n",
    "\n",
    "        p = re.compile('[0-9]+')\n",
    "        data=p.sub('', data)\n",
    "\n",
    "        data = re.sub(r\"won't\", \"will not\", data)\n",
    "        data = re.sub(r\"can\\'t\", \"can not\", data)\n",
    "\n",
    "        # general\n",
    "        data = re.sub(r\"n\\'t\", \" not\", data)\n",
    "        data = re.sub(r\"\\'re\", \" are\", data)\n",
    "        data = re.sub(r\"\\'s\", \" is\", data)\n",
    "        data = re.sub(r\"\\'d\", \" would\", data)\n",
    "        data = re.sub(r\"\\'ll\", \" will\", data)\n",
    "        data = re.sub(r\"\\'t\", \" not\", data)\n",
    "        data = re.sub(r\"\\'ve\", \" have\", data)\n",
    "        data = re.sub(r\"\\'m\", \" am\", data)\n",
    "\n",
    "#         data = ' '.join(e for e in data.split() if e.lower() not in stopwords)\n",
    "        # data=data.lower()\n",
    "        data=' '.join(data.split())\n",
    "        return_data.append(data)\n",
    "        \n",
    "    return return_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6622b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"final_df\")\n",
    "df_valid = pd.read_csv(\"final_valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146af1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['captions']=preprocess(data_df['captions'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7ad407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['enc_input'] = 'SOS '+data_df['captions']\n",
    "data_df['enc_output'] = data_df['captions']+' EOS'\n",
    "\n",
    "df_valid['enc_input'] = 'SOS '+df_valid['captions']\n",
    "df_valid['enc_output'] = df_valid['captions']+' EOS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90fa4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_df[['image','enc_input']]\n",
    "y_train = data_df['enc_output']\n",
    "\n",
    "X_test = df_valid[['image','enc_input']]\n",
    "y_test = df_valid['enc_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5785b482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['many aircraft are parked next to a long building in an airport EOS SOS many aircraft are parked next to a long building in an airport',\n",
       "       'many planes are parked next to a long building at an airport EOS SOS many planes are parked next to a long building at an airport',\n",
       "       'many planes are parked next to a long building in an airport EOS SOS many planes are parked next to a long building in an airport',\n",
       "       ...,\n",
       "       'two baseball grounds are next to a road and a lake EOS SOS two baseball grounds are next to a road and a lake',\n",
       "       'two baseball fields are next to a road and a pool EOS SOS two baseball fields are next to a road and a pool',\n",
       "       'two baseball fields are next to a road and a lake EOS SOS two baseball fields are next to a road and a lake'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.values+' '+X_train['enc_input'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36c235f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(y_train.values+' '+X_train['enc_input'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e246d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer1 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab198e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer1.word_index[\"sos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87663c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_captions = [len(i.split()) for i in X_train['enc_input'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "138b338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('F:\\Data Science\\data\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3776dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_inp_vocab_size=max(tokenizer1.index_word.keys())\n",
    "embedding_matrix = np.zeros((eng_inp_vocab_size+1, 100))\n",
    "for word, i in tokenizer1.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eab51db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data,model_im_ex, embedding, tokenizer_eng, max_len, vocab_size):\n",
    "        self.decoder_inps = data[0]['enc_input'].values\n",
    "        self.images = data[0]['image'].values\n",
    "        self.model_im_ex = model_im_ex\n",
    "#         self.decoder_inps = data['decoder_input_hin'].values\n",
    "        self.decoder_outs = data[1].values\n",
    "        self.embedding = embedding\n",
    "#         self.embedding1 = embedding1\n",
    "        self.tknizer_eng = tokenizer_eng\n",
    "#         self.tknizer_hin_inp = tokenizer_hin_inp\n",
    "#         self.tknizer_hin = tokenizer_hin\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size=vocab_size\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        img = cv2.imread(self.images[i])\n",
    "        img = cv2.resize(img,(224,224),interpolation=cv2.INTER_AREA)\n",
    "        img = img/255.0\n",
    "        \n",
    "        img = self.model_im_ex(np.array([img]))[0]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.decoder_seq = self.tknizer_eng.texts_to_sequences([self.decoder_inps[i]]) # need to pass list of values\n",
    "#         self.decoder_inp_seq = self.tknizer_hin_inp.texts_to_sequences([self.decoder_inps[i]])\n",
    "        self.decoder_out_seq = self.tknizer_eng.texts_to_sequences([self.decoder_outs[i]])\n",
    "#         print(self.decoder_inp_seq)\n",
    "        self.decoder_seq = pad_sequences(self.decoder_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        self.decoder_out_seq = pad_sequences(self.decoder_out_seq, maxlen=self.max_len, dtype='int32', padding='post')\n",
    "        \n",
    "#         self.decoder_out_seq = pad_sequences([self.decoder_inp_seq[0][1:]], maxlen=25, dtype='int32', padding='post')\n",
    "        #print(self.decoder_inp_seq1)\n",
    "#         print(self.decoder_seq)\n",
    "#         print(self.decoder_out_seq)\n",
    "        #print(self.encoder_seq)\n",
    "        self.decoder_seq1=[]\n",
    "        for j in self.decoder_seq[0]:\n",
    "            self.decoder_seq1.append(self.embedding[j])\n",
    "            \n",
    "            \n",
    "#         self.decoder_out_seq1=[]\n",
    "#         for i in self.decoder_out_seq[0]:\n",
    "#             self.decoder_out_seq1.append(self.embedding[i])    \n",
    "        #print(np.array(encoder_seq1).shape)\n",
    "        \n",
    "#         decoder_inp_seq1 =[]\n",
    "        \n",
    "#         for i in self.decoder_inp_seq[0]:\n",
    "#             decoder_inp_seq1.append(self.embedding1[i])\n",
    "            \n",
    "#         self.decoder_out_seq1 = np.zeros((25,self.vocab_hindi+1),dtype=np.float32)\n",
    "        \n",
    "#         for i,d in enumerate(self.decoder_out_seq[0]):\n",
    "#             self.decoder_out_seq1[i,d]=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(self.encoder_seq.shape)\n",
    "#         print(img)\n",
    "        return np.array(img), np.array(self.decoder_seq1), np.array(self.decoder_out_seq)\n",
    "\n",
    "    def __len__(self): # your model.fit_gen requires this function\n",
    "        return len(self.encoder_inps)\n",
    "\n",
    "    \n",
    "class Dataloder(tf.keras.utils.Sequence):    \n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        #print(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.dataset.decoder_inps))\n",
    "        #print(self.indexes)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        #print(i,'in dataloader')\n",
    "        start = i * self.batch_size\n",
    "        stop = (i + 1) * self.batch_size\n",
    "        data = []\n",
    "        for j in range(start, stop):\n",
    "            #print(self.dataset[j])\n",
    "            data.append(self.dataset[j])\n",
    "            #break\n",
    "        #print(len(data),'in dataloader')\n",
    "        batch = [np.stack(samples, axis=0) for samples in zip(*data)]\n",
    "#         print('batch')\n",
    "        #print(batch[2].shape)\n",
    "        # we are creating data like ([italian, english_inp], english_out) these are already converted into seq\n",
    "        return tuple([[batch[0],batch[1]],tf.squeeze(batch[2])])\n",
    "\n",
    "    def __len__(self):  # your model.fit_gen requires this function\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.random.permutation(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be2d1ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer1.index_word)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96be8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81f6541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_tensor=layers.Input((224,224,3)),\n",
    ")\n",
    "base_model.trainable = False\n",
    "\n",
    "input1 = base_model.input\n",
    "output1 = base_model.layers[-1].output\n",
    "\n",
    "image_feature_model = tf.keras.Model(input1,output1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66294149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "image_feature_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7bd8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f304b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset((X_train,y_train),image_feature_model, embedding_matrix, tokenizer1, 20,vocab_size)\n",
    "test_dataset  = Dataset((X_test,y_test),image_feature_model,embedding_matrix, tokenizer1,20,vocab_size)\n",
    "BATCH_SIZE=16\n",
    "train_dataloader = Dataloder(train_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = Dataloder(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d340949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 7, 7, 512)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78aa21d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader[0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4169bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 20, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[0][0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1f5b913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader[0][1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e935e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20,), dtype=int32, numpy=\n",
       "array([  6, 187,   2,  48,  49,  20,   1, 119,  37,  11,  26,  54,   3,\n",
       "         0,   0,   0,   0,   0,   0,   0])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0e6f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "#         self.vgg = VGG16(\n",
    "#                             include_top=False,\n",
    "#                             weights=\"imagenet\",\n",
    "#                             input_tensor=layers.Input(embedding_dim),\n",
    "#                         )\n",
    "        self.flat = layers.Flatten()\n",
    "        self.fc = tf.keras.layers.Dense(1000)\n",
    "        self.fc1 = tf.keras.layers.Dense(100)\n",
    "\n",
    "    def call(self, x):\n",
    "#         x = self.vgg(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.fc1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef8508c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.0003\n",
      "Epoch 1 Batch 100 Loss 1.0423\n",
      "Epoch 1 Batch 200 Loss 1.7116\n",
      "Epoch 1 Batch 300 Loss 2.2042\n",
      "Epoch 1 Batch 400 Loss 2.6954\n",
      "Epoch 1 Batch 500 Loss 2.3899\n",
      "Epoch 1 Batch 600 Loss 2.3675\n",
      "Epoch 1 Batch 700 Loss 2.0291\n",
      "Epoch 1 Batch 800 Loss 1.7893\n",
      "Epoch 1 Batch 900 Loss 0.9339\n",
      "Epoch 1 Batch 1000 Loss 3.8961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     69\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (batch, (img_tensor, target)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m#         print(batch)\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m#         print(img_tensor[0].shape)\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m#         print(target.shape)\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         batch_loss, t_loss \u001b[38;5;241m=\u001b[39m train_step(img_tensor[\u001b[38;5;241m0\u001b[39m], target)\n\u001b[0;32m     76\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t_loss\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\data_utils.py:483\u001b[0m, in \u001b[0;36mSequence.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\data_utils.py:483\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    482\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 483\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))):\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "Cell \u001b[1;32mIn[13], line 85\u001b[0m, in \u001b[0;36mDataloder.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     82\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start, stop):\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;66;03m#print(self.dataset[j])\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m#break\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m#print(len(data),'in dataloader')\u001b[39;00m\n\u001b[0;32m     88\u001b[0m batch \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mstack(samples, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata)]\n",
      "Cell \u001b[1;32mIn[13], line 63\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_seq1\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding[j])\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#         self.decoder_out_seq1=[]\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m#         for i in self.decoder_out_seq[0]:\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m#             self.decoder_out_seq1.append(self.embedding[i])    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m#print(self.encoder_seq.shape)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#         print(img)\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_seq1), np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_out_seq)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding_dim = (224,224,3)\n",
    "units = 1024\n",
    "vocab_size = vocab_size\n",
    "num_steps = len(X_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64\n",
    "\n",
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(100, units, vocab_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "loss_plot = []\n",
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "#   print(target.shape)\n",
    "\n",
    "  dec_input = tf.expand_dims([tokenizer1.word_index['sos']] * target.shape[0], 1)\n",
    "#   print(dec_input)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "#           print(target[:, i])\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "#           print(dec_input)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss\n",
    "\n",
    "import time\n",
    "start_epoch = 0\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataloader):\n",
    "#         print(batch)\n",
    "#         print(img_tensor[0].shape)\n",
    "#         print(target.shape)\n",
    "        batch_loss, t_loss = train_step(img_tensor[0], target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9deebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image, max_length):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['sos']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == 'endseq':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"test4.jpg\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "img = cv2.resize(img,(224,224),interpolation=cv2.INTER_AREA)\n",
    "img = img/255.0\n",
    "Argmax_Search = evaluate(img,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
